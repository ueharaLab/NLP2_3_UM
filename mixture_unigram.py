
import pandas as pd
import numpy as np
import em_algorithm
from sklearn.decomposition import PCA
import codecs
import matplotlib
import matplotlib.pyplot as plt
import japanize_matplotlib
import pickle
import seaborn as sns
matplotlib.pyplot.rcParams['figure.figsize'] = (16.0, 10.0)

#### æ··åˆã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†å¸ƒã‚¯ãƒ©ã‚¹ã®å®Ÿè¡Œ


with open('dict_inv.pickle', 'rb') as f:# {èªå½™:id} ã®ã‚ˆã†ãªè¾æ›¸
    dict_inv = pickle.load(f)
with open('dataset.pickle', 'rb') as f:# å£ã‚³ãƒŸæ¯ã®å‡ºç¾èªå½™idã®ãƒªã‚¹ãƒˆï¼ˆ2æ¬¡å…ƒé…åˆ—ï¼‰ bowå½¢å¼ã§ã¯ãªã„
    all_reviews = pickle.load(f)

word_dic = {i:w for w, i in dict_inv.items()}# {id:èªå½™} ã®ã‚ˆã†ãªè¾æ›¸
no_of_topics = 5
em_cat=em_algorithm.em_categorical(all_reviews,dict_inv,no_of_topics)# EM algorithmã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆ
passes =20
theta,phi,qdk =em_cat.fit(passes) # E-step, M-stepã‚’å®Ÿè¡Œã—ã¦ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šçµæœã‚’è¿”ã™ã€€passes=ã¯E-step, M-stepã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°


# ===== ã‚¯ãƒ©ã‚¹ã‚¿ğœ™_ğ‘˜ã®ç‰¹å¾´ã‚’è¡¨ç¤ºã™ã‚‹ã€€ ãƒˆãƒ”ãƒƒã‚¯ğœ™_ğ‘˜ğ‘£ã‚’ç¢ºç‡ã®é™é †ã«ä¸¦ã¹ã¦æ›¸ãå‡ºã™ã¨ã€ãã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚‰ã—ã•ã®èªå½™é †ã«è¡¨ç¤ºã§ãã‚‹
topic=[]
for row in phi: # å•é¡Œï¼šphi ã®æ¬¡å…ƒæ•°ã¯ã„ãã¤ã‹ï¼Ÿï¼ˆãƒ’ãƒ³ãƒˆï¼šdict_invã¯ã€å‡ºç¾èªå½™ã®è¾æ›¸ï¼‰
    w_dic = {word_dic[i]:np.round(val,2) for i,val in enumerate(row)}
    w_dic_sort = (sorted(w_dic.items(), reverse=True, key=lambda x:x[1]))
    print(w_dic_sort[:20])
    topic.append(w_dic_sort)
topic_df = pd.DataFrame(topic)
with codecs.open("fortravel_unigram_topics.csv", "w", "ms932", "ignore") as f: 
    #header=Trueã§ã€è¦‹å‡ºã—ã‚’æ›¸ãå‡ºã™
    topic_df.to_csv(f, index=False, encoding="ms932", mode='w')

# =====  è² æ‹…ç‡ğ‘_ğ‘‘ğ‘˜ ãŒæœ€å¤§å€¤ã¨ãªã‚‹ã‚¯ãƒ©ã‚¹ã‚¿k ã‚’å£ã‚³ãƒŸdæ¯ã«æ±‚ã‚ã‚‹ã€‚np.argmax(pdk,axis=1)ã¯ä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã‹ï¼Ÿ
 
clusters = np.argmax(qdk,axis =1)

# =====  ä¸Šè¨˜ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ã€å£ã‚³ãƒŸãƒ‡ãƒ¼ã‚¿ã«ãƒ©ãƒ™ãƒ«ä»˜ã‘ã™ã‚‹ã€€

fortravel = pd.read_csv('origin_data_filter.csv', encoding='ms932', sep=',',skiprows=0)
assert len(clusters)==len(fortravel), 'unmatch cluster and dataset'
fortravel['cluster']=clusters.reshape(-1,1)
with codecs.open("fortravel_cluster.csv", "w", "ms932", "ignore") as f: 
    #header=Trueã§ã€è¦‹å‡ºã—ã‚’æ›¸ãå‡ºã™
    fortravel.to_csv(f, index=False, encoding="ms932", mode='w')


#### Unigram Mixtureã¯ã“ã“ã¾ã§ã€‚ã€€ä»¥ä¸‹PCAï¼ˆèª¬æ˜ã¯çœç•¥ï¼‰ ##############################################################################




with open('bow.pickle', 'rb') as f:
    fortravel_bow = pickle.load(f)
feature_matrix = fortravel_bow.values
dict_word = fortravel_bow.columns.tolist()
# BOWã‚’PCAã™ã‚‹
pca = PCA(n_components=10)
	
# æ•™å¸«ãªã—ãªã®ã§ã€fitã®å¼•æ•°ã¯ç‰¹å¾´é‡ã®ã¿ã«ãªã£ã¦ã„ã‚‹ï¼ˆæ•™å¸«ãƒ©ãƒ™ãƒ«ãŒãªã„ï¼‰
pca.fit(feature_matrix)
pca_matrix=pca.components_
pca_score_matrix = pca.transform(feature_matrix)
feature_pca_vectors=pca.components_.T


#ç‰¹å¾´é‡è¡Œåˆ—ã®ä¸»æˆåˆ†å¾—ç‚¹ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆåˆ—æ–¹å‘ãŒä¸»æˆåˆ†ï¼‰   
pca_score_matrix = pca.transform(feature_matrix)

#ä¸»æˆåˆ†ãƒ™ã‚¯ãƒˆãƒ«è¡Œåˆ—ã‚’è»¢ç½®ã™ã‚‹ï¼ˆè¡Œã¯èªå½™ç‰¹å¾´é‡ã€€åˆ—ã¯ç¬¬ä¸€ä¸»æˆåˆ†ã‹ã‚‰ã€‚ã€‚ï¼‰
feature_pca_vectors=pca.components_.T
#èªå½™ã®ä¸»æˆåˆ†ã®1ï¼Œ2åˆ—ç›®ï¼ˆç¬¬ä¸€ä¸»æˆåˆ†ã€ç¬¬2ä¸»æˆåˆ†ï¼‰ã‚’å–ã‚Šå‡ºã™
feature_pca_df = pd.DataFrame(feature_pca_vectors[:,0:2])

#dataframeã®åˆ—ã«è¦‹å‡ºã—è¡Œã‚’ã¤ã‘ã‚‹
feature_pca_df.columns = ['PC1','PC2']
#ç¬¬ä¸€ä¸»æˆåˆ†ã€ç¬¬äºŒä¸»æˆåˆ†ã®äºŒä¹—å’Œã‚’è¦‹å‡ºã—ã€sq_sumã§dataframeã«è¿½åŠ 
square_df = pd.DataFrame(feature_pca_df['PC1']**2+feature_pca_df['PC2']**2)
square_df.columns = ['sq_sum']
feature_name_df=pd.DataFrame(dict_word)
feature_name_df.columns = ['words']
#èªå½™ã€ä¸»æˆåˆ†ã€äºŒä¹—å’Œã‚’ã¤ãªã’ãŸdataframeã‚’ä½œã‚‹
feature_pca_df = pd.concat([feature_name_df,feature_pca_df,square_df], axis=1)
#ä¸»æˆåˆ†å€¤ã®å¤§ãã„èªå½™é †ã«ã‚½ãƒ¼ãƒˆ
feature_pca_df= feature_pca_df.sort_values(by='sq_sum', ascending=False)
j=len(feature_pca_df)
#10è¡Œç›®ã¾ã§ã§ã‚¹ãƒ©ã‚¤ã‚¹
feature_pca_df=feature_pca_df[0:20]


#å£ã‚³ãƒŸãƒ™ã‚¯ãƒˆãƒ«ã®ä¸»æˆåˆ†å¾—ç‚¹è¡Œåˆ—ã®ç¬¬ä¸€ä¸»æˆåˆ†ã€ç¬¬äºŒä¸»æˆåˆ†ã‚’å–ã‚Šå‡ºã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆ
pca_score_df = pd.DataFrame(pca_score_matrix[:,0:2])
#pca_score_df = pca_score_df.sample(n=500)
#ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¦‹å‡ºã—ã‚’ä»˜ã‘ã‚‹
pca_score_df.columns = ['PC1','PC2']
pca_score_df = pd.concat([pca_score_df,fortravel['cluster']],axis=1)
#2æ¬¡å…ƒä¸»æˆåˆ†ç©ºé–“ä¸Šã«å£ã‚³ãƒŸï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ï¼‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã€‚ã¾ãŸã€ä¸Šè¨˜ã§æ±‚ã‚ãŸä¸»æˆåˆ†ã®å€¤ãŒå¤§ãã„èªå½™ç‰¹å¾´é‡ã‚‚åŒæ™‚ã«ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹
#ax = pca_score_df.plot(kind='scatter', x='PC2', y='PC1',style=['ro', 'bs'],s=5, alpha=0.2, figsize=(40,10))
current_palette = sns.color_palette(n_colors=5)
sns.scatterplot(x='PC2', y='PC1',  data=pca_score_df,hue='cluster', alpha = 0.7, s=100, palette=current_palette)
#ä¸Šè¨˜ã¨åŒã˜å¹³é¢ä¸Šã«ã€ä¸»æˆåˆ†å€¤ã®å¤§ãã„èªå½™ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹

c=0
for word, pca2,pca1 in zip(feature_pca_df['words'],feature_pca_df['PC2'],feature_pca_df['PC1']):
    #èªå½™ã®ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒ—ãƒ­ãƒƒãƒˆã«èªå½™ãƒ©ãƒ™ãƒ«ã‚’ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹
    plt.arrow(0,0,pca2*4.5,pca1*4.5,width=0.002,head_width=0.01,head_length=0.04,length_includes_head=True,color='blue')
    plt.annotate(word,xy=(pca2*5.5,pca1*5.),size=14,color='blue')
    
    
plt.show()



'''






pc_x=0
pc_y=1
pcx='PC'+str(pc_x+1)
pcy='PC'+str(pc_y+1)
feature_pca_df = pd.DataFrame(feature_pca_vectors[:,pc_x:pc_y+1])
print(feature_pca_df)
#dataframeã®åˆ—ã«è¦‹å‡ºã—è¡Œã‚’ã¤ã‘ã‚‹
feature_pca_df.columns = [pcx,pcy]
#ç¬¬ä¸€ä¸»æˆåˆ†ã€ç¬¬äºŒä¸»æˆåˆ†ã®äºŒä¹—å’Œã‚’è¦‹å‡ºã—ã€sq_sumã§dataframeã«è¿½åŠ 
square_df = pd.DataFrame(feature_pca_df[pcx]**2+feature_pca_df[pcy]**2)
square_df.columns = ['sq_sum']
feature_name_df=pd.DataFrame(dict_word)
feature_name_df.columns = ['words']
#èªå½™ã€ä¸»æˆåˆ†ã€äºŒä¹—å’Œã‚’ã¤ãªã’ãŸdataframeã‚’ä½œã‚‹
feature_pca_df = pd.concat([feature_name_df,feature_pca_df,square_df], axis=1)
#ä¸»æˆåˆ†å€¤ã®å¤§ãã„èªå½™é †ã«ã‚½ãƒ¼ãƒˆ
feature_pca_df= feature_pca_df.sort_values(by='sq_sum', ascending=False)
j=len(feature_pca_df)
#10è¡Œç›®ã¾ã§ã§ã‚¹ãƒ©ã‚¤ã‚¹
feature_pca_df=feature_pca_df[0:10]
print(feature_pca_df)
pca_score_df = pd.DataFrame(pca_score_matrix[:,pc_x:pc_y+1])
pca_score_df.columns = [pcx,pcy]

colors={i:rgb for i,(color, rgb) in enumerate(matplotlib.colors.CSS4_COLORS.items())}
color_dict = {label:colors[i] for i, label in enumerate(np.unique(clusters))}
color_df = pd.DataFrame([color_dict[label] for label in clusters])
color_df.columns = ['color']
pca_score_df=pd.concat([pca_score_df,color_df],axis=1)
	
ax = pca_score_df.plot(kind='scatter', x=pcx, y=pcy,style=['ro', 'bs'],s=10, c=pca_score_df['color'],alpha=1, figsize=(40,10))
#ä¸Šè¨˜ã¨åŒã˜å¹³é¢ä¸Šã«ã€ä¸»æˆåˆ†å€¤ã®å¤§ãã„èªå½™ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹
for clusterId, px,py in zip(clusters,pca_score_df[pcx],pca_score_df[pcy]):
	ax.annotate(clusterId,xy=(px,py),size=6)
	
for word, pca1,pca2 in zip(feature_pca_df['words'],feature_pca_df[pcx],feature_pca_df[pcy]):
#for word, pca1,pca2 in zip(clusters,feature_pca_df[pcx],feature_pca_df[pcy]):

	ax.annotate(word,xy=(pca1*3,pca2*3),size=10,color='blue')
	ax.quiver(0,0,pca1*3,pca2*3,width=0.002,angles='xy',color='blue',scale_units='xy',scale=1)
		
for i,(k,v) in 	enumerate(color_dict.items()):
	ax.text(2.2, 2.8+(i*0.2), k, size=8, color=v)		
plt.show()

print('å„æ¬¡å…ƒã®å¯„ä¸ç‡: {0}'.format(pca.explained_variance_ratio_))
print('ç´¯ç©å¯„ä¸ç‡: {0}'.format(sum(pca.explained_variance_ratio_)))	

	#qdk argmaxã—ã¦ã€€ä¸»æˆåˆ†å¹³é¢ä¸Šã«ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹
'''